"""
Speech recognizer for Groq Speech services.

This module provides a comprehensive speech recognition system that integrates
with Groq's AI-powered speech-to-text and translation APIs. It supports both
single-shot and continuous recognition modes with advanced audio processing
capabilities.

ARCHITECTURE OVERVIEW:
1. CORE COMPONENTS
   - SpeechRecognizer: Main class for speech recognition operations
   - TimingMetrics: Performance tracking and timing measurements
   - SpeechRecognitionResult: Structured results with metadata
   - CancellationDetails/NoMatchDetails: Error handling and diagnostics

2. AUDIO PROCESSING PIPELINE
   - Audio preprocessing (resampling, mono conversion)
   - Chunking for large files
   - Voice Activity Detection (VAD)
   - Audio compression and optimization

3. API INTEGRATION
   - Groq transcription API for speech-to-text
   - Groq translation API for speech-to-translation
   - Configurable model parameters and response formats
   - Error handling and retry logic

4. EVENT SYSTEM
   - Event-driven architecture for real-time processing
   - Handler registration for recognition events
   - Session management and lifecycle events

KEY FEATURES:
- Real-time microphone input with configurable chunk sizes
- File-based audio processing with automatic chunking
- Automatic language detection (no manual language specification needed)
- Word and segment-level timestamps
- Performance metrics and timing analysis
- Configurable audio quality and processing parameters
- Support for both transcription and translation modes
- Thread-safe continuous recognition
- Comprehensive error handling and diagnostics

USAGE EXAMPLES:
    # Basic transcription
    config = SpeechConfig()
    recognizer = SpeechRecognizer(config)
    result = recognizer.recognize_once_async()

    # Translation to English
    config.enable_translation = True
    recognizer = SpeechRecognizer(config)
    result = recognizer.translate_audio_data(audio_data)

    # Continuous recognition
    recognizer.start_continuous_recognition()
    # ... handle events ...
    recognizer.stop_continuous_recognition()
"""

import time
import threading
import io
from typing import Optional, Callable, List, Dict, Any
import numpy as np
import soundfile as sf  # type: ignore
import groq
from .speech_config import SpeechConfig
from .audio_config import AudioConfig
from .audio_processor import OptimizedAudioProcessor, AudioChunker
from .result_reason import ResultReason, CancellationReason
from .config import Config
from .property_id import PropertyId
from .speaker_diarization import SpeakerDiarizer, DiarizationConfig, DiarizationResult


class TimingMetrics:
    """
    Timing metrics for transcription pipeline.

    CRITICAL: This class provides detailed performance profiling for the entire
    speech recognition pipeline. It tracks timing at each stage to help identify
    bottlenecks and optimize performance:

    1. Microphone Capture: Time spent recording audio from microphone
    2. API Call: Time spent making the actual Groq API request
    3. Response Processing: Time spent parsing and processing the API response
    4. Total Time: End-to-end processing time

    The metrics are essential for:
    - Performance optimization and debugging
    - User experience monitoring
    - API response time analysis
    - Identifying audio processing bottlenecks
    """

    def __init__(self):
        """Initialize timing metrics with all timestamps set to None."""
        self.microphone_start = None
        self.microphone_end = None
        self.api_call_start = None
        self.api_call_end = None
        self.processing_start = None
        self.processing_end = None
        self.total_start = None
        self.total_end = None

    def start_microphone(self):
        """Start microphone timing and set total start time if not
        already set."""
        self.microphone_start = time.time()
        if not self.total_start:
            self.total_start = self.microphone_start

    def end_microphone(self):
        """End microphone timing."""
        self.microphone_end = time.time()

    def start_api_call(self):
        """Start API call timing."""
        self.api_call_start = time.time()

    def end_api_call(self):
        """End API call timing."""
        self.api_call_end = time.time()

    def start_processing(self):
        """Start response processing timing."""
        self.processing_start = time.time()

    def end_processing(self):
        """End response processing timing and set total end time."""
        self.processing_end = time.time()
        self.total_end = time.time()

    def get_metrics(self) -> Dict[str, float]:
        """
        Get all timing metrics as a dictionary.

        Returns:
            Dictionary containing timing measurements for each pipeline stage.
            Only includes metrics that have been measured (start and end
            # times set).
        """
        metrics = {}

        if self.microphone_start and self.microphone_end:
            metrics["microphone_capture"] = self.microphone_end - self.microphone_start

        if self.api_call_start and self.api_call_end:
            metrics["api_call"] = self.api_call_end - self.api_call_start

        if self.processing_start and self.processing_end:
            metrics["response_processing"] = self.processing_end - self.processing_start

        if self.total_start and self.total_end:
            metrics["total_time"] = self.total_end - self.total_start

        return metrics


class SpeechRecognitionResult:
    """
    Result of a speech recognition operation.

    CRITICAL: This class encapsulates all information returned from a speech
    recognition operation. It provides a consistent
    # interface for accessing
    recognition results, errors, and metadata:

    1. Recognition Content: Text, confidence, and detected language
    2. Result Status: Success, no match, or cancellation with detailed reasons
    3. Timing Information: Performance metrics for the entire pipeline
    4. Metadata: Word/segment timestamps, audio processing details

    The result object is used throughout the system to:
    - Return recognition results to calling applications
    - Provide detailed error information for debugging
    - Track performance metrics and timing
    - Support both transcription and translation modes
    """

    def __init__(
        self,
        text: str = "",
        reason: ResultReason = ResultReason.NoMatch,
        confidence: float = 0.0,
        language: str = "",
        cancellation_details: Optional["CancellationDetails"] = None,
        no_match_details: Optional["NoMatchDetails"] = None,
        timestamps: Optional[List[Dict[str, Any]]] = None,
        timing_metrics: Optional[TimingMetrics] = None,
    ):
        """
        Initialize speech recognition result.

        Args:
            text: Recognized text from speech input
            reason: Result reason indicating success/failure status
            confidence: Confidence score (0.0 to 1.0) for the recognition
            language: Detected or specified language of the speech
            cancellation_details: Details if recognition was canceled
            no_match_details: Details if no speech was recognized
            timestamps: Word-level or segment-level timestamps with metadata
            timing_metrics: Performance metrics for the transcription pipeline
        """
        self.text = text
        self.reason = reason
        self.confidence = confidence
        self.language = language
        self.cancellation_details = cancellation_details
        self.no_match_details = no_match_details
        self.timestamps = timestamps or []
        self.timing_metrics = timing_metrics

    def __str__(self):
        """String representation for debugging and logging."""
        return f"SpeechRecognitionResult(text='{self.text}', reason={self.reason}, confidence={self.confidence})"


class CancellationDetails:
    """
    Details about why speech recognition was canceled.

    CRITICAL: This class provides detailed information about recognition failures
    and cancellations. It's essential for debugging and user feedback:

    1. Cancellation Reason: Categorized reason for the failure
    2. Error Details: Specific error message or description
    3. Debugging Support: Helps developers identify and fix issues
    4. User Experience: Provides meaningful error messages to users

    Common cancellation reasons include:
    - API errors (network, authentication, rate limiting)
    - Audio processing failures (format, quality, corruption)
    - Configuration errors (invalid settings, missing credentials)
    - System errors (memory, threading, resource issues)
    """

    def __init__(self, reason: CancellationReason, error_details: str = ""):
        """
        Initialize cancellation details.

        Args:
            reason: Categorized reason for cancellation
            error_details: Detailed error message for debugging
        """
        self.reason = reason
        self.error_details = error_details


class NoMatchDetails:
    """
    Details about why no speech was recognized.

    CRITICAL: This class provides information when the system cannot recognize
    any speech in the audio input. It helps users understand why recognition failed:

    1. No Match Reason: Categorized reason for no recognition
    2. Error Details: Specific details about the failure
    3. User Guidance: Helps users improve audio quality or settings
    4. Debugging Support: Assists in troubleshooting recognition issues

    Common no-match reasons include:
    - Audio too quiet or background noise too loud
    - Unsupported language or dialect
    - Audio format or quality issues
    - Microphone or hardware problems
    """

    def __init__(self, reason: str = "NoMatch", error_details: str = ""):
        """
        Initialize no match details.

        Args:
            reason: Categorized reason for no match
            error_details: Detailed explanation of why no speech was recognized
        """
        self.reason = reason
        self.error_details = error_details


class SpeechRecognizer:
    """
    Main class for speech recognition using Groq services.

    CRITICAL: This is the core class that orchestrates the entire speech recognition
    pipeline. It handles audio input, processing, API calls, and result management:

    1. Audio Processing: Preprocessing, chunking, and optimization
    2. API Integration: Groq transcription and translation endpoints
    3. Event Management: Real-time event handling and callbacks
    4. Performance Tracking: Metrics, timing, and statistics
    5. Error Handling: Comprehensive error management and recovery

    The recognizer supports multiple modes:
    - Single-shot recognition (recognize_once_async)
    - Continuous recognition (start_continuous_recognition)
    - File-based processing (recognize_audio_data)
    - Translation mode (translate_audio_data)

    Key architectural features:
    - Thread-safe operations for concurrent recognition
    - Event-driven architecture for real-time processing
    - Configurable audio processing parameters
    - Automatic language detection and handling
    - Performance monitoring and optimization
    """

    def __init__(
        self, speech_config: SpeechConfig, audio_config: Optional[AudioConfig] = None
    ):
        """
        Initialize speech recognizer with configuration.

        CRITICAL: This initialization sets up the entire recognition pipeline and
        validates all configurations. It's essential for proper operation:

        Args:
            speech_config: Speech configuration including API keys and model settings
            audio_config: Audio configuration for microphone and file processing

        Initialization process:
        1. Configuration validation and setup
        2. Groq client initialization with error handling
        3. Audio processor setup with optimized parameters
        4. Event handler system initialization
        5. Performance tracking and statistics setup
        6. State management for continuous recognition
        """
        self.speech_config = speech_config
        self.audio_config = audio_config

        # Initialize Groq client with proper error handling
        try:
            # For newer versions of groq library, only pass api_key
            self.groq_client = groq.Groq(api_key=speech_config.api_key)
        except Exception as e:
            raise Exception(f"Failed to initialize Groq client: {e}")

        # Initialize optimized audio processor with configuration
        audio_config_dict = Config.get_audio_config()
        self.audio_processor = OptimizedAudioProcessor(
            sample_rate=audio_config_dict["sample_rate"],
            channels=audio_config_dict["channels"],
            chunk_duration=audio_config_dict["chunk_duration"],
            buffer_size=audio_config_dict["buffer_size"],
            enable_vad=audio_config_dict["vad_enabled"],
            enable_compression=audio_config_dict["enable_compression"],
        )

        # Initialize audio chunker for large files with overlap
        self.audio_chunker = AudioChunker(
            chunk_duration=30.0,  # 30 seconds per chunk for optimal processing
            overlap_duration=2.0,  # 2 seconds overlap to prevent word cutting
            sample_rate=audio_config_dict["sample_rate"],
        )

        # Event handlers for real-time processing
        self.recognizing_handlers: List[Callable] = []
        self.recognized_handlers: List[Callable] = []
        self.session_started_handlers: List[Callable] = []
        self.session_stopped_handlers: List[Callable] = []
        self.canceled_handlers: List[Callable] = []

        # Continuous recognition state management
        self._is_recognizing = False
        self._recognition_thread = None
        self._stop_recognition = False

        # Performance tracking and statistics
        self.performance_stats = {
            "total_requests": 0,
            "total_processing_time": 0.0,
            "avg_response_time": 0.0,
            "successful_recognitions": 0,
            "failed_recognitions": 0,
        }

        # Validate configuration before proceeding
        self.speech_config.validate()

    def _preprocess_audio(self, audio_data: np.ndarray) -> np.ndarray:
        """
        Preprocess audio data for Groq API requirements.

        CRITICAL: This method ensures audio data meets Groq's API specifications
        and optimizes it for best recognition results:

        Args:
            audio_data: Input audio data as numpy array

        Returns:
            Preprocessed audio data ready for API submission

        Preprocessing steps:
        1. Mono conversion: Convert stereo to mono if needed
        2. Resampling: Ensure 16kHz sample rate (Groq requirement)
        3. Format validation: Check audio quality and dimensions
        4. Normalization: Ensure proper audio levels and range
        """
        # Ensure audio is mono (Groq API requirement)
        if len(audio_data.shape) > 1 and audio_data.shape[1] > 1:
            audio_data = np.mean(audio_data, axis=1)

        # Resample to 16kHz if needed (Groq API requirement)
        target_sample_rate = 16000
        current_sample_rate = (
            self.audio_config.sample_rate if self.audio_config else 16000
        )

        if current_sample_rate != target_sample_rate:
            # Simple resampling (in production, use librosa or scipy for better quality)
            ratio = target_sample_rate / current_sample_rate
            new_length = int(len(audio_data) * ratio)
            audio_data = np.interp(
                np.linspace(0, len(audio_data), new_length),
                np.arange(len(audio_data)),
                audio_data,
            )

        return audio_data

    def _call_groq_transcription_api(
        self, audio_buffer: io.BytesIO, is_translation: bool = False
    ) -> Any:
        """
        Call Groq API for transcription or translation with optimized parameters.

        CRITICAL: This method handles the actual API communication with Groq's
        speech services. It's the core of the recognition system:

        Args:
            audio_buffer: Audio data buffer in WAV format
            is_translation: Whether to use translation endpoint

        Returns:
            API response as dictionary or response object

        API call process:
        1. Parameter preparation with environment overrides
        2. Model selection and configuration
        3. Timestamp granularity setup
        4. Endpoint selection (transcription vs translation)
        5. Error handling and response validation
        6. Performance tracking and timing
        """
        try:
            # Get model configuration from environment with defaults
            model_config = Config.get_model_config()

            # Get configuration parameters with environment overrides
            model = model_config["model_id"]
            response_format = model_config["response_format"]
            temperature = model_config["temperature"]
            prompt = (
                self.speech_config.get_property(PropertyId.Speech_Recognition_Prompt)
                or None
            )

            # Prepare timestamp granularities based on configuration
            timestamp_granularities = []
            if model_config["enable_word_timestamps"]:
                timestamp_granularities.append("word")
            if model_config["enable_segment_timestamps"]:
                timestamp_granularities.append("segment")

            # Default to segment if no granularities specified
            if not timestamp_granularities:
                timestamp_granularities = ["segment"]

            # Prepare API parameters for optimal recognition
            api_params = {
                "file": ("audio.wav", audio_buffer.getvalue(), "audio/wav"),
                "model": model,
                "response_format": response_format,
                "timestamp_granularities": timestamp_granularities,
                "temperature": temperature,
            }

            # Language auto-detected by Groq API - no need to specify
            # This provides better accuracy and supports all languages

            # Add prompt if specified for context-aware recognition
            if prompt:
                api_params["prompt"] = prompt

            # Call appropriate API endpoint based on configuration
            # Check if translation is enabled in config or passed as parameter
            should_translate = is_translation or getattr(
                self.speech_config, "enable_translation", False
            )

            if should_translate:
                # For translation, use only the basic parameters supported by Groq translations API
                translation_params = {
                    "file": ("audio.wav", audio_buffer.getvalue(), "audio/wav"),
                    "model": model,
                    "response_format": response_format,
                    "temperature": temperature,
                }

                # Add prompt if specified for context-aware translation
                if prompt:
                    translation_params["prompt"] = prompt

                return self.groq_client.audio.translations.create(**translation_params)
            else:
                return self.groq_client.audio.transcriptions.create(**api_params)

        except Exception as e:
            raise Exception(f"Groq API call failed: {str(e)}")

    def _parse_groq_response(
        self, response: Any, is_translation: bool = False
    ) -> SpeechRecognitionResult:
        """
        Parse Groq API response into SpeechRecognitionResult.

        CRITICAL: This method converts the raw API response into a structured
        result object that provides consistent access to recognition data:

        Args:
            response: Raw Groq API response object
            is_translation: Whether this was a translation response

        Returns:
            Parsed recognition result with all metadata

        Parsing process:
        1. Text extraction and validation
        2. Confidence score processing
        3. Language detection and handling
        4. Timestamp extraction and formatting
        5. Error handling for malformed responses
        6. Result object construction with metadata
        """
        try:
            # Extract text from response
            text = getattr(response, "text", "")

            # Extract confidence score (if available, default to 0.95)
            confidence = getattr(response, "confidence", 0.95)

            # Extract language information
            if is_translation:
                # For translation, try to get the detected source language from the response
                # The translated text is in English, but we want to show what language was spoken
                language = getattr(response, "language", None)

                # If no language in response, try to get it from segments
                if not language and hasattr(response, "segments"):
                    for segment in response.segments:
                        if hasattr(segment, "language") and segment.language:
                            language = segment.language
                            break

                # If still no language, try to detect from the response object attributes
                if not language:
                    # Try common attribute names for source language
                    for attr_name in [
                        "source_language",
                        "input_language",
                        "detected_language",
                    ]:
                        if hasattr(response, attr_name):
                            language = getattr(response, attr_name)
                            break

                # If no language detected, show "Auto-detected" to indicate Groq handled it
                if not language:
                    language = "Auto-detected"
            else:
                language = getattr(
                    response, "language", self.speech_config.speech_recognition_language
                )

            # Extract timestamps if verbose_json format is used
            timestamps = []
            if hasattr(response, "segments"):
                for segment in response.segments:
                    timestamp_info = {
                        "start": getattr(segment, "start", 0),
                        "end": getattr(segment, "end", 0),
                        "text": getattr(segment, "text", ""),
                        "avg_logprob": getattr(segment, "avg_logprob", 0),
                        "compression_ratio": getattr(segment, "compression_ratio", 0),
                        "no_speech_prob": getattr(segment, "no_speech_prob", 0),
                    }
                    timestamps.append(timestamp_info)

            # Construct and return the result object
            return SpeechRecognitionResult(
                text=text,
                reason=ResultReason.RecognizedSpeech,
                confidence=confidence,
                language=language,
                timestamps=timestamps,
            )

        except Exception as e:
            # Return error result if parsing fails
            return SpeechRecognitionResult(
                reason=ResultReason.Canceled,
                cancellation_details=CancellationDetails(
                    CancellationReason.Error, f"Failed to parse API response: {str(e)}"
                ),
            )

    def connect(self, event_type: str, handler: Callable):
        """
        Connect an event handler for real-time processing.

        CRITICAL: This method enables the event-driven architecture that allows
        applications to respond to recognition events in real-time:

        Args:
            event_type: Type of event to handle
            handler: Function to call when event occurs

        Supported event types:
        - 'recognizing': Fired during active recognition
        - 'recognized': Fired when recognition completes successfully
        - 'session_started': Fired when recognition session begins
        - 'session_stopped': Fired when recognition session ends
        - 'canceled': Fired when recognition is canceled or fails

        Event handling enables:
        - Real-time UI updates and progress indicators
        - Streaming recognition results
        - Session management and lifecycle tracking
        - Error handling and user feedback
        """
        if event_type == "recognizing":
            self.recognizing_handlers.append(handler)
        elif event_type == "recognized":
            self.recognized_handlers.append(handler)
        elif event_type == "session_started":
            self.session_started_handlers.append(handler)
        elif event_type == "session_stopped":
            self.session_stopped_handlers.append(handler)
        elif event_type == "canceled":
            self.canceled_handlers.append(handler)
        else:
            raise ValueError(f"Unknown event type: {event_type}")

    def _trigger_event(self, event_type: str, event_data: Any):
        """
        Trigger event handlers for a specific event type.

        CRITICAL: This method executes all registered handlers for a given event,
        enabling the event-driven architecture. It includes error handling to
        prevent one handler failure from affecting others.
        """
        handlers = []
        if event_type == "recognizing":
            handlers = self.recognizing_handlers
        elif event_type == "recognized":
            handlers = self.recognized_handlers
        elif event_type == "session_started":
            handlers = self.session_started_handlers
        elif event_type == "session_stopped":
            handlers = self.session_stopped_handlers
        elif event_type == "canceled":
            handlers = self.canceled_handlers

        # Execute all handlers with error isolation
        for handler in handlers:
            try:
                handler(event_data)
            except Exception as e:
                print(f"Error in event handler: {e}")

    def recognize_once_async(self) -> "SpeechRecognitionResult":
        """
        Perform single-shot speech recognition.

        CRITICAL: This is the primary method for speech recognition. It handles
        the complete recognition pipeline from audio input to result:

        Returns:
            SpeechRecognitionResult object with recognition data

        Recognition pipeline:
        1. Session initialization and event triggering
        2. Audio input (file or microphone)
        3. Audio processing and preprocessing
        4. API call to Groq services
        5. Response parsing and result construction
        6. Session cleanup and event triggering
        7. Performance tracking and statistics
        """
        try:
            # Trigger session started event for lifecycle tracking
            self._trigger_event(
                "session_started", {"session_id": f"session_{int(time.time())}"}
            )

            # Get audio data from configured source
            if self.audio_config and self.audio_config.filename:
                # File-based recognition with optimized processing
                audio_data = self.audio_config.get_file_audio_data()
                result = self.recognize_audio_data(audio_data)
            else:
                # Microphone-based recognition with real-time processing
                result = self._recognize_from_microphone()

            # Trigger session stopped event for cleanup
            self._trigger_event(
                "session_stopped", {"session_id": f"session_{int(time.time())}"}
            )

            return result

        except Exception as e:
            # Return error result with cancellation details
            cancellation_details = CancellationDetails(
                CancellationReason.Error, f"Recognition failed: {str(e)}"
            )
            return SpeechRecognitionResult(
                reason=ResultReason.Canceled, cancellation_details=cancellation_details
            )

    def recognize_audio_data(
        self, audio_data: np.ndarray, is_translation: bool = False
    ) -> SpeechRecognitionResult:
        """
        Recognize speech from audio data using Groq API.

        CRITICAL: This method processes pre-recorded audio data and is the core
        of the recognition system. It handles the complete pipeline:

        Args:
            audio_data: Audio data as numpy array
            is_translation: Whether to use translation endpoint

        Returns:
            SpeechRecognitionResult with recognition data and timing metrics

        Processing pipeline:
        1. Timing metrics initialization and tracking
        2. Audio preprocessing (format, sample rate, channels)
        3. Audio buffer preparation (WAV format for API)
        4. Groq API call with optimized parameters
        5. Response parsing and result construction
        6. Performance metrics calculation and storage
        7. Error handling and recovery
        """
        timing_metrics = TimingMetrics()

        try:
            # Start API call timing for performance measurement
            timing_metrics.start_api_call()

            # Preprocess audio for API requirements
            audio_data = self._preprocess_audio(audio_data)

            # Save audio to temporary buffer in WAV format
            buffer = io.BytesIO()
            sf.write(
                buffer, audio_data, 16000, format="WAV"
            )  # Use 16kHz as per Groq requirements
            buffer.seek(0)

            # Call Groq API with timing measurement
            response = self._call_groq_transcription_api(buffer, is_translation)

            # End API call timing
            timing_metrics.end_api_call()

            # Start processing timing for response handling
            timing_metrics.start_processing()

            # Parse API response into structured result
            result = self._parse_groq_response(response, is_translation)

            # End processing timing
            timing_metrics.end_processing()

            # Add timing metrics to result for performance analysis
            result.timing_metrics = timing_metrics

            return result

        except Exception as e:
            # Handle errors with timing metrics and cancellation details
            timing_metrics.end_processing()
            cancellation_details = CancellationDetails(
                CancellationReason.Error, f"Recognition failed: {str(e)}"
            )
            result = SpeechRecognitionResult(
                reason=ResultReason.Canceled,
                cancellation_details=cancellation_details,
                timing_metrics=timing_metrics,
            )
            return result

    def translate_audio_data(self, audio_data: np.ndarray) -> SpeechRecognitionResult:
        """
        Translate audio to English text using Groq API.

        CRITICAL: This method provides speech-to-translation functionality,
        converting speech in any language to English text:

        Args:
            audio_data: Audio data as numpy array

        Returns:
            SpeechRecognitionResult with English translation

        Translation process:
        1. Audio preprocessing and optimization
        2. Groq translation API call
        3. Response parsing and validation
        4. Result construction with translation metadata
        5. Performance tracking and timing
        """
        return self.recognize_audio_data(audio_data, is_translation=True)

    def _recognize_from_microphone(self) -> SpeechRecognitionResult:
        """
        Recognize speech from microphone input.

        CRITICAL: This method handles real-time microphone input for live
        speech recognition. It's optimized for single-shot recognition:

        Returns:
            SpeechRecognitionResult with recognition data

        Microphone recognition process:
        1. Audio configuration setup and validation
        2. Real-time audio capture with progress indication
        3. Audio chunk collection and buffering
        4. Audio data conversion and normalization
        5. Recognition processing with timing metrics
        6. Error handling and user feedback
        7. Performance statistics update
        """
        if not self.audio_config:
            self.audio_config = AudioConfig()

        timing_metrics = TimingMetrics()
        timing_metrics.start_microphone()

        try:
            print("Speak into your microphone...")
            print("(Press Ctrl+C to stop)")

            # For single-shot mode, we want to capture complete audio without early cutoff
            # The frontend handles the start/stop recording, so we just collect until interrupted
            audio_chunks = []
            start_time = time.time()
            max_duration = 120  # Increased to 2 minutes for complete speech capture

            # Use the audio config to read audio with optimized settings
            with self.audio_config as audio:
                print("Recording audio...")
                print("(Press Ctrl+C to stop recording)")

                while time.time() - start_time < max_duration:
                    try:
                        # Read audio chunk from microphone
                        chunk = audio.read_audio_chunk(
                            4096
                        )  # Increased chunk size for efficiency
                        if chunk and len(chunk) > 0:
                            # Collect all audio chunks without silence detection for single-shot mode
                            audio_chunks.append(chunk)
                            print(".", end="", flush=True)  # Show progress indicator

                    except KeyboardInterrupt:
                        print("\nStopped by user")
                        break
                    except Exception as e:
                        print(f"\nError reading audio: {e}")
                        break

                print()  # New line after progress dots

                if not audio_chunks:
                    print(
                        "No audio captured. Please try speaking louder or check your microphone."
                    )
                    timing_metrics.end_microphone()
                    return SpeechRecognitionResult(
                        reason=ResultReason.NoMatch,
                        no_match_details=NoMatchDetails("No audio captured"),
                        timing_metrics=timing_metrics,
                    )

                print(f"Captured {len(audio_chunks)} audio chunks")
                timing_metrics.end_microphone()

                # Combine audio chunks into single audio stream
                audio_data = b"".join(audio_chunks)

                # Convert to numpy array with proper data type and normalization
                audio_array = np.frombuffer(audio_data, dtype=np.int16)
                audio_array_float = (
                    audio_array.astype(np.float32) / 32768.0
                )  # Normalize to [-1, 1] range

                # Update performance statistics
                self.performance_stats["total_requests"] += 1

                # Process the audio data through the recognition pipeline
                return self.recognize_audio_data(audio_array_float)

        except Exception as e:
            # Handle errors with proper cleanup and error reporting
            timing_metrics.end_microphone()
            self.performance_stats["failed_recognitions"] += 1
            cancellation_details = CancellationDetails(
                CancellationReason.Error, f"Microphone recognition failed: {str(e)}"
            )
            return SpeechRecognitionResult(
                reason=ResultReason.Canceled,
                cancellation_details=cancellation_details,
                timing_metrics=timing_metrics,
            )

    def start_continuous_recognition(self):
        """
        Start continuous speech recognition.

        CRITICAL: This method initiates continuous recognition mode, which
        continuously processes audio input and fires events for real-time
        applications. It's essential for:

        1. Real-time transcription applications
        2. Live captioning and subtitling
        3. Continuous monitoring and analysis
        4. Interactive voice applications

        The method ensures thread safety and proper state management.
        """
        if self._is_recognizing:
            return

        self._is_recognizing = True
        self._stop_recognition = False
        self._recognition_thread = threading.Thread(
            target=self._continuous_recognition_worker
        )
        self._recognition_thread.start()

    def stop_continuous_recognition(self):
        """
        Stop continuous speech recognition.

        CRITICAL: This method safely stops continuous recognition and ensures
        proper cleanup of resources and threads. It's essential for:

        1. Resource cleanup and memory management
        2. Thread safety and proper termination
        3. State consistency and error prevention
        4. Application shutdown and cleanup
        """
        self._stop_recognition = True
        self._is_recognizing = False
        if self._recognition_thread:
            self._recognition_thread.join()

    def _continuous_recognition_worker(self):
        """
        Worker thread for continuous recognition.

        CRITICAL: This method runs in a separate thread and handles the
        continuous recognition loop. It's responsible for:

        1. Continuous audio processing and recognition
        2. Event triggering for real-time applications
        3. Error handling and recovery
        4. Performance monitoring and optimization
        5. Thread safety and state management
        """
        if not self.audio_config:
            self.audio_config = AudioConfig()

        timing_metrics = TimingMetrics()
        timing_metrics.start_microphone()

        try:
            print("ðŸŽ¤ Continuous recognition started - speak naturally...")

            with self.audio_config as audio:
                # Buffer to accumulate audio data with proper overlap to prevent word loss
                audio_buffer = []

                # Get chunking configuration from environment
                chunking_config = Config.get_chunking_config()
                buffer_duration = chunking_config["buffer_duration"]
                overlap_duration = chunking_config["overlap_duration"]
                chunk_size = chunking_config["chunk_size"]

                samples_per_second = 16000  # 16kHz sample rate
                buffer_size = int(buffer_duration * samples_per_second)
                overlap_size = int(overlap_duration * samples_per_second)

                while not self._stop_recognition:
                    try:
                        # Read audio chunk from microphone
                        chunk = audio.read_audio_chunk(
                            chunk_size
                        )  # Use configurable chunk size
                        if chunk and len(chunk) > 0:
                            audio_buffer.append(chunk)

                            # Check if we have enough audio data to process
                            total_samples = sum(
                                len(c) // 2 for c in audio_buffer
                            )  # 16-bit = 2 bytes

                            if total_samples >= buffer_size:
                                # Combine all chunks and process
                                combined_audio = b"".join(audio_buffer)
                                audio_array = np.frombuffer(
                                    combined_audio, dtype=np.int16
                                )
                                audio_array_float = (
                                    audio_array.astype(np.float32) / 32768.0
                                )

                                # Process the audio data
                                result = self.recognize_audio_data(audio_array_float)

                                # Update performance stats
                                self.performance_stats["total_requests"] += 1

                                if result.reason == ResultReason.RecognizedSpeech:
                                    self.performance_stats[
                                        "successful_recognitions"
                                    ] += 1
                                    self._trigger_event("recognized", result)
                                elif result.reason == ResultReason.Canceled:
                                    self._trigger_event("canceled", result)
                                    break
                                elif result.reason == ResultReason.NoMatch:
                                    # Continue listening for speech
                                    pass

                                # Keep overlap portion to prevent word loss
                                # Calculate how many chunks we need to keep for overlap
                                overlap_bytes = overlap_size * 2  # 16-bit = 2 bytes
                                current_bytes = 0
                                chunks_to_keep = 0

                                # Find how many chunks we need to keep for overlap
                                for i in range(len(audio_buffer) - 1, -1, -1):
                                    current_bytes += len(audio_buffer[i])
                                    if current_bytes >= overlap_bytes:
                                        chunks_to_keep = i
                                        break

                                # Keep the overlap portion and remove the rest
                                audio_buffer = audio_buffer[chunks_to_keep:]

                    except Exception as e:
                        # Handle audio stream errors gracefully
                        if "Input overflowed" in str(e):
                            # Audio buffer overflow - continue listening
                            continue
                        elif "Stream closed" in str(e) or "Stream not open" in str(e):
                            # Stream closed - try to restart
                            try:
                                audio.start_microphone_stream()
                                continue
                            except:
                                break
                        else:
                            print(f"Error in continuous recognition: {e}")
                            # Small delay before retrying
                            time.sleep(0.1)
                            continue

        except Exception as e:
            print(f"Error in continuous recognition worker: {e}")
        finally:
            timing_metrics.end_microphone()
            self._trigger_event(
                "session_stopped", {"session_id": f"session_{int(time.time())}"}
            )

    def recognize_once(self) -> SpeechRecognitionResult:
        """
        Perform single-shot speech recognition (synchronous).

        CRITICAL: This method provides a synchronous interface for recognition,
        useful for simple applications that don't need async processing.

        Returns:
            SpeechRecognitionResult object
        """
        return self.recognize_once_async()

    def is_recognizing(self) -> bool:
        """
        Check if recognition is currently active.

        CRITICAL: This method provides thread-safe access to the recognition
        state, essential for:

        1. UI state management and updates
        2. Resource management and cleanup
        3. Error handling and recovery
        4. Application state consistency
        """
        return self._is_recognizing

    def get_performance_stats(self) -> dict:
        """
        Get comprehensive performance statistics.

        CRITICAL: This method provides detailed performance metrics for
        monitoring, optimization, and debugging. It includes:

        1. Recognition success/failure rates
        2. Processing time and response time metrics
        3. Audio processing performance data
        4. Configuration and model information
        5. System resource utilization

        Returns:
            Dictionary with comprehensive performance metrics
        """
        audio_stats = self.audio_processor.get_performance_stats()

        return {
            "total_requests": self.performance_stats["total_requests"],
            "successful_recognitions": self.performance_stats[
                "successful_recognitions"
            ],
            "failed_recognitions": self.performance_stats["failed_recognitions"],
            "avg_response_time": self.performance_stats["avg_response_time"],
            "audio_processing": audio_stats,
            "model_config": Config.get_model_config(),
            "audio_config": Config.get_audio_config(),
        }

    def recognize_with_diarization(
        self,
        audio_data: np.ndarray,
        diarization_config: Optional[DiarizationConfig] = None,
        sample_rate: int = 16000,
        is_translation: bool = False,
    ) -> DiarizationResult:
        """
        Perform speech recognition with speaker diarization.

        CRITICAL: This method combines speaker diarization with speech
        recognition, providing a complete solution for multi-speaker
        audio processing:

        Args:
            audio_data: Input audio data as numpy array
            diarization_config: Optional diarization configuration
            sample_rate: Sample rate of the audio data
            is_translation: Whether to use translation endpoint

        Returns:
            DiarizationResult with speaker segments and transcribed/translated text

        Processing pipeline:
        1. Speaker diarization and segmentation
        2. Individual segment transcription/translation via Groq API
        3. Text assignment and confidence scoring
        4. Result compilation and validation
        """
        try:
            # Create diarizer with speech configuration
            diarizer = SpeakerDiarizer(
                config=diarization_config, speech_config=self.speech_config
            )

            # Check if diarization is available
            if not hasattr(diarizer, "_pipeline") or diarizer._pipeline is None:
                print(
                    "âš ï¸  Pyannote.audio diarization not available, falling back to basic transcription..."
                )
                return self._fallback_basic_transcription(
                    audio_data, sample_rate, is_translation
                )

            # Perform diarization first
            result = diarizer.diarize_audio(audio_data, sample_rate, diarization_config)

            if not result.is_successful:
                print("âš ï¸  Diarization failed, falling back to basic transcription...")
                return self._fallback_basic_transcription(
                    audio_data, sample_rate, is_translation
                )

            # Now transcribe or translate each segment
            mode = "translation" if is_translation else "transcription"
            print(f"ðŸ”„ Starting {mode} for {len(result.segments)} segments...")

            for i, segment in enumerate(result.segments):
                try:
                    # Extract audio for this segment
                    start_sample = int(segment.start_time * sample_rate)
                    end_sample = int(segment.end_time * sample_rate)
                    segment_audio = audio_data[start_sample:end_sample]

                    # Transcribe or translate segment
                    if is_translation:
                        text = self._transcribe_audio_segment(
                            segment_audio, is_translation=True
                        )
                        segment.text = text
                        segment.transcription_confidence = 0.95  # Default confidence
                    else:
                        text = self._transcribe_audio_segment(
                            segment_audio, is_translation=False
                        )
                        segment.text = text
                        segment.transcription_confidence = 0.95  # Default confidence

                except Exception as e:
                    print(f"Error processing segment {i}: {e}")
                    error_msg = f"[{mode.capitalize()} Error]"
                    segment.text = error_msg
                    segment.transcription_confidence = 0.0

            return result

        except Exception as e:
            print(f"âš ï¸  Diarization failed: {e}, falling back to basic transcription...")
            return self._fallback_basic_transcription(
                audio_data, sample_rate, is_translation
            )

    def _fallback_basic_transcription(
        self, audio_data: np.ndarray, sample_rate: int, is_translation: bool = False
    ) -> DiarizationResult:
        """
        Fallback to basic transcription/translation when diarization is not available.

        This method provides basic speech recognition without speaker diarization,
        treating the entire audio as a single speaker segment.

        Args:
            audio_data: Input audio data as numpy array
            sample_rate: Sample rate of the audio data
            is_translation: If True, translate to English; if False, transcribe

        Returns:
            DiarizationResult with a single speaker segment
        """
        try:
            print("ðŸ”„ Performing basic transcription/translation...")

            # Transcribe or translate the entire audio
            if is_translation:
                text = self._transcribe_audio_segment(audio_data, is_translation=True)
                mode = "translation"
            else:
                text = self._transcribe_audio_segment(audio_data, is_translation=False)
                mode = "transcription"

            # Create a single speaker segment
            from .speaker_diarization import SpeakerSegment

            segment = SpeakerSegment(
                start_time=0.0,
                end_time=len(audio_data) / sample_rate,
                speaker_id="speaker_1",
                text=text,
                transcription_confidence=0.95,
            )

            # Create speaker mapping
            speaker_mapping = {"speaker_1": "Speaker"}

            # Create result
            from .speaker_diarization import DiarizationResult

            result = DiarizationResult(
                segments=[segment],
                speaker_mapping=speaker_mapping,
                total_duration=len(audio_data) / sample_rate,
                num_speakers=1,
                overall_confidence=0.95,
                processing_time=0.0,
            )

            print(f"âœ… Basic {mode} completed successfully")
            return result

        except Exception as e:
            print(f"âŒ Fallback transcription failed: {e}")
            # Return error result
            from .speaker_diarization import DiarizationResult

            return DiarizationResult(
                segments=[],
                speaker_mapping={},
                total_duration=len(audio_data) / sample_rate,
                num_speakers=0,
                overall_confidence=0.0,
                processing_time=0.0,
                error_details=f"Fallback transcription failed: {str(e)}",
            )

    def _transcribe_audio_segment(
        self, audio_data: np.ndarray, is_translation: bool = False
    ) -> str:
        """
        Transcribe or translate an audio segment using Groq API.

        Args:
            audio_data: Audio data for the segment
            is_translation: If True, translate to English; if False, transcribe

        Returns:
            Transcribed or translated text
        """
        try:
            # Preprocess audio for API requirements
            audio_data = self._preprocess_audio(audio_data)

            # Save audio to temporary buffer in WAV format
            buffer = io.BytesIO()
            sf.write(buffer, audio_data, 16000, format="WAV")
            buffer.seek(0)

            # Call appropriate Groq API endpoint
            if is_translation:
                # Use translation API
                response = self.groq_client.audio.translations.create(
                    file=("audio.wav", buffer.getvalue(), "audio/wav"),
                    model="whisper-large-v3",  # Use v3 for translation
                    response_format="text",
                    temperature=0.0,
                )
            else:
                # Use transcription API
                response = self.groq_client.audio.transcriptions.create(
                    file=("audio.wav", buffer.getvalue(), "audio/wav"),
                    model="whisper-large-v3-turbo",  # Use turbo for transcription
                    response_format="text",
                    temperature=0.0,
                )

            # Handle different response formats
            if hasattr(response, "text"):
                return response.text
            elif isinstance(response, str):
                return response
            else:
                # Try to get text from response object
                return str(response)

        except Exception as e:
            error_type = "translation" if is_translation else "transcription"
            print(f"Error calling Groq {error_type} API: {e}")
            return f"[{error_type.capitalize()} Error]"

    def recognize_with_correct_diarization(
        self, audio_file: str, mode: str
    ) -> "DiarizationResult":
        """
        CORRECT PIPELINE: Use proper diarization with accurate transcription.

        This method implements the correct architecture:
        1. Pyannote.audio detects speakers FIRST
        2. Audio is split into speaker-specific chunks
        3. Each chunk is transcribed by Groq API
        4. Perfect speaker attribution with accurate text

        Args:
            audio_file: Path to audio file
            mode: 'transcription' or 'translation'

        Returns:
            DiarizationResult with accurate speaker-specific transcriptions
        """
        print(f"ðŸŽ­ Running CORRECT diarization pipeline for {mode}...")

        try:
            from .speaker_diarization import SpeakerDiarizer

            # Create diarizer with current configuration
            diarizer = SpeakerDiarizer()

            # Use the correct pipeline method
            result = diarizer.diarize_with_accurate_transcription(
                audio_file=audio_file, mode=mode, speech_recognizer=self
            )

            return result

        except Exception as e:
            print(f"âŒ CORRECT diarization failed: {e}")
            print("ðŸ”„ Falling back to basic transcription...")

            # Fallback: basic transcription without diarization
            try:
                # Load audio file and use recognize_audio_data
                import soundfile as sf

                audio_data, sample_rate = sf.read(audio_file)

                # Convert to mono if stereo
                if len(audio_data.shape) > 1:
                    audio_data = audio_data[:, 0]

                # Resample to 16kHz if needed
                if sample_rate != 16000:
                    from scipy import signal

                    audio_data = signal.resample(
                        audio_data, int(len(audio_data) * 16000 / sample_rate)
                    )

                # Use the correct method
                if mode == "translation":
                    basic_result = self.translate_audio_data(audio_data)
                else:
                    basic_result = self.recognize_audio_data(audio_data)

                if basic_result and basic_result.text:
                    print(f"âœ… Basic {mode} completed: {basic_result.text[:100]}...")
                    return basic_result
                else:
                    print(f"âŒ Basic {mode} also failed")
                    return None

            except Exception as audio_error:
                print(f"âŒ Audio loading failed: {audio_error}")
                return None
