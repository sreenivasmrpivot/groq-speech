# Groq Speech SDK

A comprehensive speech recognition and translation SDK with speaker diarization capabilities, featuring both CLI and web interfaces.

## üöÄ **Quick Start**

### **Prerequisites**
- Python 3.8+
- Node.js 18+ (for web UI)
- Groq API key
- Hugging Face token (for diarization)

### **Installation**

1. **Clone the repository:**
```bash
git clone <repository-url>
cd groq-speech
```

2. **Install Python dependencies:**
```bash
pip install -r requirements.txt
```

3. **Install frontend dependencies:**
```bash
cd examples/groq-speech-ui
npm install
```

4. **Configure environment:**
```bash
cp groq_speech/env.template groq_speech/.env
# Edit groq_speech/.env with your API keys
```

### **Usage**

#### **CLI Interface (Direct SDK Access)**
```bash
# File transcription
python examples/speech_demo.py --file audio.wav

# File transcription with diarization
python examples/speech_demo.py --file audio.wav --diarize

# Microphone single mode
python examples/speech_demo.py --microphone-mode single

# Microphone continuous mode with diarization
python examples/speech_demo.py --microphone-mode continuous --diarize

# Translation mode
python examples/speech_demo.py --file audio.wav --operation translation --diarize
```

#### **Web Interface (REST API)**
```bash
# Start API server
cd api && python server.py

# Start frontend (in another terminal)
cd examples/groq-speech-ui && npm run dev

# Open http://localhost:3000
```

## üèóÔ∏è **Architecture**

### **3-Layer Architecture**

```mermaid
graph TB
    subgraph "Layer 3: UI Client"
        UI[groq-speech-ui/<br/>EnhancedSpeechDemo.tsx<br/>PerformanceMetrics.tsx]
    end
    
    subgraph "Layer 2b: API Client"
        API[api/server.py<br/>FastAPI REST API only]
    end
    
    subgraph "Layer 2a: CLI Client"
        CLI[speech_demo.py<br/>Command Line Interface]
    end
    
    subgraph "Layer 1: SDK"
        SDK[groq_speech/<br/>speech_recognizer.py<br/>speaker_diarization.py<br/>vad_service.py<br/>audio_utils.py]
    end
    
    UI -->|HTTP REST| API
    CLI -->|Direct Calls| SDK
    API -->|Direct Calls| SDK
    
    style UI fill:#1976D2,color:#ffffff
    style API fill:#7B1FA2,color:#ffffff
    style CLI fill:#7B1FA2,color:#ffffff
    style SDK fill:#388E3C,color:#ffffff
```

### **Key Components**

#### **Core SDK (`groq_speech/`)**
- **`speech_recognizer.py`** - Main orchestrator, handles all speech processing
- **`speech_config.py`** - Configuration management with factory methods
- **`speaker_diarization.py`** - Speaker diarization using Pyannote.audio
- **`vad_service.py`** - Voice Activity Detection service
- **`audio_utils.py`** - Audio format utilities and conversion
- **`exceptions.py`** - Custom exception classes
- **`result_reason.py`** - Result status enums

#### **API Server (`api/`)**
- **`server.py`** - FastAPI server with REST endpoints only
- **`models/`** - Pydantic request/response models
- **REST API** - HTTP endpoints for all operations

#### **Frontend (`examples/groq-speech-ui/`)**
- **`EnhancedSpeechDemo.tsx`** - Main UI component with all features
- **`audio-recorder.ts`** - Unified audio recording (standard + optimized)
- **`continuous-audio-recorder.ts`** - VAD-based continuous recording
- **`client-vad-service.ts`** - Client-side Voice Activity Detection
- **`audio-converter.ts`** - Unified audio conversion (standard + optimized)
- **`groq-api.ts`** - REST API client

## üîÑ **Data Flow**

### **CLI Flow (Direct Access)**
```
Audio Input ‚Üí numpy array ‚Üí SDK Processing ‚Üí Console Output
```

### **Web UI Flow (REST API)**
```
Audio Input ‚Üí Frontend Processing ‚Üí HTTP REST ‚Üí API Server ‚Üí SDK Processing ‚Üí JSON Response ‚Üí UI Display
```

### **Audio Format Handling**
- **File Processing**: Base64-encoded WAV ‚Üí HTTP REST ‚Üí base64 decode ‚Üí numpy array
- **Microphone Processing**: Float32Array ‚Üí HTTP REST ‚Üí array conversion ‚Üí numpy array
- **VAD Processing**: Client-side for real-time performance

## üéØ **Features**

### **Speech Recognition**
- ‚úÖ File-based transcription
- ‚úÖ Microphone single mode
- ‚úÖ Microphone continuous mode with VAD
- ‚úÖ Real-time audio level visualization
- ‚úÖ Silence detection and chunking

### **Translation**
- ‚úÖ File-based translation
- ‚úÖ Microphone translation
- ‚úÖ Multi-language support
- ‚úÖ Target language configuration

### **Speaker Diarization**
- ‚úÖ Pyannote.audio integration
- ‚úÖ GPU acceleration support
- ‚úÖ Multi-speaker detection
- ‚úÖ Speaker-specific segments

### **Voice Activity Detection (VAD)**
- ‚úÖ Client-side real-time processing
- ‚úÖ 15-second silence detection
- ‚úÖ Audio level visualization
- ‚úÖ Automatic chunk creation

### **Performance Optimizations**
- ‚úÖ Unified audio recorders (standard + optimized)
- ‚úÖ Unified audio converters (standard + optimized)
- ‚úÖ Client-side VAD for real-time processing
- ‚úÖ Chunked processing for large files
- ‚úÖ Memory-efficient operations

## üîå **API Endpoints**

### **Core Endpoints**
- `POST /api/v1/recognize` - File transcription
- `POST /api/v1/translate` - File translation
- `POST /api/v1/recognize-microphone` - Single microphone processing
- `POST /api/v1/recognize-microphone-continuous` - Continuous microphone processing

### **Utility Endpoints**
- `GET /health` - Health check
- `GET /api/v1/models` - Available models
- `GET /api/v1/languages` - Supported languages
- `POST /api/log` - Frontend logging

### **VAD Endpoints (Legacy)**
- `POST /api/v1/vad/should-create-chunk` - VAD chunk detection
- `POST /api/v1/vad/audio-level` - Audio level analysis

## üê≥ **Deployment**

### **Docker (Local Development)**
```bash
# Standard deployment
docker-compose -f deployment/docker/docker-compose.yml up

# GPU-enabled deployment
docker-compose -f deployment/docker/docker-compose.gpu.yml up

# Development with hot reload
docker-compose -f deployment/docker/docker-compose.dev.yml up
```

### **GCP Cloud Run (Production)**
```bash
# Deploy to Cloud Run with GPU support
cd deployment/gcp
./deploy.sh
```

## üìä **Performance**

### **CLI Performance**
- **Direct SDK access** - No network overhead
- **Real-time VAD** - Local processing
- **Memory efficient** - Direct numpy array handling

### **Web UI Performance**
- **Client-side VAD** - Real-time silence detection
- **Unified components** - Optimized for both short and long audio
- **Chunked processing** - Handles large files efficiently
- **REST API** - Scalable and maintainable

## üîß **Configuration**

### **Environment Variables**
```bash
# Required
GROQ_API_KEY=your_groq_api_key

# Optional (for diarization)
HF_TOKEN=your_huggingface_token

# Optional (for GPU support)
CUDA_VISIBLE_DEVICES=0
```

### **Audio Settings**
- **Sample Rate**: 16kHz (standard)
- **Channels**: Mono (1 channel)
- **Format**: Float32Array (microphone), WAV (files)
- **VAD Threshold**: 0.003 RMS (conservative detection)

## üìö **Documentation**

- **[Architecture Guide](docs/ARCHITECTURE.md)** - Detailed system architecture
- **[API Reference](groq_speech/API_REFERENCE.md)** - SDK API documentation
- **[Deployment Guide](deployment/README.md)** - Deployment instructions
- **[Code Analysis](docs/CODE_ANALYSIS.md)** - Detailed code analysis

## ü§ù **Contributing**

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## üìÑ **License**

This project is licensed under the MIT License - see the LICENSE file for details.

## üÜò **Support**

For issues and questions:
1. Check the [documentation](docs/)
2. Review [existing issues](https://github.com/your-repo/issues)
3. Create a new issue with detailed information

---

**Built with ‚ù§Ô∏è using Groq, Pyannote.audio, and modern web technologies.**